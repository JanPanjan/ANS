\document{article}

\begin{document}

\hypertarget{general-instructions}{%
\section{General instructions}\label{general-instructions}}

Instructions for filling in the worksheets:

\begin{itemize}
\item
  Worksheets include questions (some are for refreshing the knowledge,
  whereas some will probably require a little bit of literature review)
  and practical work, where you carefully insert all codes and as much
  as possible comments.
\item
  Each step of the bioinformatics analysis should be well documented and
  explained why it was performed and why each parameter/option of the
  command was used.
\item
  Bioinformatics tools which you will use have several options. I
  encourage you to explore why they are used for. Make sure that you add
  this to the report as well.
\item
  Comments regarding the worksheets, how can be improved, what should be
  added, etc. are welcome.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{setting-up-your-environment}{%
\section{Setting up your
environment}\label{setting-up-your-environment}}

\textbf{Virtual environments} are great, because they let you have
separate environments for separate projects. This is advantageous, since
one project could rely on a certain package version 3, while some other
may require version 4.

\hypertarget{conda}{%
\subsection{Conda}\label{conda}}

An advantage that \textbf{Conda} provides is not only for managing
Python libraries, but also command line tools. This can make the tool's
instalation process uniform and more generalized for users that don't
work on the same systems.

I recommend installing Conda with these instructions:
\url{docs.conda.io/projects/conda/en/latest/user-guide/install/index.html}.
Essentially the difference between Miniconda and Anaconda is that with
Miniconda you have to install many tools manually. Install whichever you
like.

\hypertarget{bioconda}{%
\subsubsection{Bioconda}\label{bioconda}}

To use certain bioinformatics tools, we need to use the
\textbf{Bioconda} channel. No installation is needed, only this 3
commands that alter your \texttt{.condarc} configuration:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ config }\AttributeTok{{-}{-}add}\NormalTok{ channels bioconda}
\ExtensionTok{conda}\NormalTok{ config }\AttributeTok{{-}{-}add}\NormalTok{ channels conda{-}forge}
\ExtensionTok{conda}\NormalTok{ config }\AttributeTok{{-}{-}set}\NormalTok{ channel\_priority strict}
\end{Highlighting}
\end{Shaded}

\hypertarget{virtual-environment}{%
\subsection{Virtual environment}\label{virtual-environment}}

Finally, we can create an environment for this project using:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ create }\AttributeTok{{-}{-}name}\NormalTok{ ans}
\end{Highlighting}
\end{Shaded}

Answer the prompt with yes to create an environment and then
\textbf{activate} the environment with \texttt{conda\ activate\ ans}.
Activation of the environment is local to a terminal session, so you
will have to use this command again if you open another terminal
instance. To deactivate it simply run \texttt{conda\ deactivate}.

\hypertarget{finding-our-data}{%
\section{Finding our data}\label{finding-our-data}}

During this course we were tasked to work on the RNA-seq dataset linked
to the article titled \emph{``Wolbachia pipientis modulates germline
stem cells and gene expression associated with ubiquitination and
histone lysine trimethylation to rescue fertility defects in
Drosophila''}.

\hypertarget{bioproject-accession-number}{%
\subsection{BioProject accession
number}\label{bioproject-accession-number}}

First thing to do is to find this article on \textbf{NCBI} or
\textbf{PubMed}. This can be done with a quick Google search:
https://academic.oup.com/genetics/article/229/3/iyae220/7934994\#510949052.

In the article, find the section \emph{Data availability}. There will be
an accession number for a \textbf{BioProject} (a BioProject is a
collection of biological data related to a large-scale research effort).

\hypertarget{sra-accession-number}{%
\subsection{SRA accession number}\label{sra-accession-number}}

We can search for sequencing data related to this BioProject through the
command line using \textbf{NCBI Entrez Direct tool}. First we install it
in our environment with
\texttt{conda\ install\ bioconda::entrez-direct}. Then we make a query
and save the output in \texttt{csv} format.

\begin{itemize}
\tightlist
\item
  \texttt{esearch} queries the database and returns all accession
  numbers that match that query
\item
  \texttt{efetch} fetches the data linked to those accession numbers
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{esearch} \AttributeTok{{-}db}\NormalTok{ sra }\AttributeTok{{-}query}\NormalTok{ PRJNA1166928 }\KeywordTok{|} \ExtensionTok{efetch} \AttributeTok{{-}format}\NormalTok{ runinfo }\OperatorTok{\textgreater{}}\NormalTok{ runinfo.csv}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-db} specifies an NCBI database to search
\item
  \texttt{-query} specifies the search query
\item
  \texttt{\textbar{}} pipes the output of the previous command as input
  for this command
\item
  \texttt{-format} specifies the output format
\end{itemize}

If we take a quick look at this file, it has many different columns,
while we are only interested in the SRA (sequence read archive)
accession numbers. We can extract the first column with \texttt{cut} on
the output of \texttt{tail} that will skip the first line (the header):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail} \AttributeTok{{-}n}\NormalTok{ +2 runinfo.csv }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}d} \StringTok{\textquotesingle{},\textquotesingle{}} \AttributeTok{{-}f1} \OperatorTok{\textgreater{}}\NormalTok{ SraAccList.txt}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-n\ +2} tells \texttt{tail} to start displaying from the
  second line
\item
  The \texttt{-d} flag is used to specify a delimiter and the
  \texttt{-f1} tells \texttt{cut} to extract the first field.
\end{itemize}

Each of us students had to choose one accession number. I choose
\textbf{SRR30833097}. Save this into a file with
\texttt{echo\ "SRR30833097"\ \textgreater{}\ OurAcc.txt}.

\hypertarget{questions-regarding-research-and-sequencing-dataset}{%
\subsection{Questions regarding research and sequencing
dataset}\label{questions-regarding-research-and-sequencing-dataset}}

\hypertarget{a-what-is-the-aim-of-the-study-described-in-the-scientific-article}{%
\subsubsection{a) What is the aim of the study, described in the
scientific
article?}\label{a-what-is-the-aim-of-the-study-described-in-the-scientific-article}}

To study how \emph{Wolbachia pipientis} influences \textbf{gene
expression} (particulary the gene expression of genes associated with
ubiquitin and histone lysine trimethylation) when rescuing the fertility
defect on female hypomorph \textbf{bam gene} (\emph{bag of marbles} - a
gene involved in germ cell differentiation).

\hypertarget{b-provide-some-info-about-the-dataset-you-will-work-with-and-which-sequencing-technology-was-used.}{%
\subsubsection{b) Provide some info about the dataset you will work with
and which sequencing technology was
used.}\label{b-provide-some-info-about-the-dataset-you-will-work-with-and-which-sequencing-technology-was-used.}}

{[}{[}Pasted image 20250421105830.png{]}{]}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Name:} - This dataset contains reads from ovarie tissue of
infected and uninfected \emph{Drosophiila menogaster} organisms with
wild-type genotypes and genotypes with hypomorphed (functional gene with
reduced function) \emph{bam} gene. - Since this is a
\href{https://en.wikipedia.org/wiki/3\%27_mRNA-seq}{3' RNA-seq}, this
means that the 3' untranslated regions of polyadenylated mRNA's were
sequenced instead of the whole trancript.

\textbf{Sequencing technology:} - ILLUMINA (NextSeq 500).

\textbf{Run statistics:} - The data comes from a single sequencing run
with the mentioned technology. - 4.5 million reads were generated. -
Total number of nucleotides across all reads is 390.2 million.

\textbf{Library:} - Source material are transcripts. - Selection
(filtering of RNA) by polyA tail was used. - SINGLE layout means our
reads were single-ended.

\hypertarget{c-which-kit-was-used-for-sequencing-library-preparation-does-this-kit-preserve-strand-information-stranded-library-or-not}{%
\subsubsection{c) Which kit was used for sequencing library preparation?
Does this kit preserve strand information (stranded library) or
not?}\label{c-which-kit-was-used-for-sequencing-library-preparation-does-this-kit-preserve-strand-information-stranded-library-or-not}}

The kit is not listed, so I cannot answer this question.

\hypertarget{d-what-is-the-advantage-of-stranded-mrna-library-preparation-compared-to-non-stranded-library}{%
\subsubsection{d) What is the advantage of stranded mRNA library
preparation compared to non-stranded
library?}\label{d-what-is-the-advantage-of-stranded-mrna-library-preparation-compared-to-non-stranded-library}}

\textbf{Stranded libraries} are prepared in a way to contain information
about the strand of cDNA from which the transcript originates.

This kind of library: - allows you to \textbf{distinguish between the}
\textbf{sense} and \textbf{antisense} strands of cDNA - is useful for -
identifying \textbf{antisense trancription} - \textbf{gene annotation}
and \textbf{novel gene discovery} - determining the \textbf{origin of
RNA} molecules in \textbf{overlapping regions} - \textbf{accurately}
quantifying gene expression

\begin{quote}
source: https://youtu.be/yp9A5E-Y49Y?si=qzVTqlXUrEowuIKG,
https://lcsciences.com/why-is-strand-specific-library-preparation-important/
\end{quote}

\hypertarget{downloading-data}{%
\section{Downloading data}\label{downloading-data}}

\hypertarget{prefetch}{%
\subsection{Prefetch}\label{prefetch}}

Sequencing data is obtained from the SRA database with the SRA Toolkit.
It can be installed with
\texttt{conda\ install\ -c\ bioconda\ sra-tools}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{prefetch} \AttributeTok{{-}{-}option{-}file}\NormalTok{ OurAcc.txt}
\end{Highlighting}
\end{Shaded}

\textbf{Prefetch} is used to obtain \emph{Runs} (sequence files in
compressed SRA format). The \texttt{-\/-output-file} flag is used to use
a file with a list of accession numbers as input. The command creates a
directory named after the given accession number, where the downloaded
files reside.

The prefetched runs can be converted into FastQ format using
\texttt{fasterq-dump}, that takes the created directory as input:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fasterq{-}dump} \AttributeTok{{-}{-}skip{-}technical}\NormalTok{ SRR30833097/}
\end{Highlighting}
\end{Shaded}

\texttt{-\/-skip-technical} returns only biological reads.

Since we have only single-end sequences, it should output a single
\texttt{.fastq} file in the current directory. You can check that the
line count matches 18149460 with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wc} \AttributeTok{{-}l}\NormalTok{ SRR30833097.fastq}
\end{Highlighting}
\end{Shaded}

\hypertarget{generating-a-quality-report}{%
\section{Generating a quality
report}\label{generating-a-quality-report}}

\hypertarget{a-run-fastqc-over-your-dataset.-explain-the-results}{%
\subsection{a) Run FastQC over your dataset. Explain the
results}\label{a-run-fastqc-over-your-dataset.-explain-the-results}}

\hypertarget{command-line-fastqc}{%
\subsubsection{Command line FastQC}\label{command-line-fastqc}}

To get a \textbf{full report} on all sequences in our dataset, we can
use \textbf{FastQC tool}. Install it with
\texttt{conda\ install\ -c\ bioconda\ fastqc} and run it on the
\texttt{.fastq} file.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastqc}\NormalTok{ SRR30833097.fastq}
\end{Highlighting}
\end{Shaded}

The quality report is the generated \texttt{.html} file. To view the
rendered file, you can open it with your browser
(e.g.~\texttt{opera\ *.html}).

\hypertarget{gui-fastqc}{%
\subsubsection{GUI FastQC}\label{gui-fastqc}}

Alternativelly, you can open a \textbf{graphical user interface} of
FastQC tool by executing only \texttt{fastqc}. This will open a new
window where you can open your \texttt{.fastq} file and generate and
view the report.

{[}{[}Pasted image 20250418203153.png{]}{]}

Click on \texttt{File\ \textgreater{}\ Open} and select your file. When
it's finished, you should see the report. Click on
\texttt{File\ \textgreater{}\ Save\ report} and save the report in the
working directory as \texttt{pre-filtertering\_fastqc.html}.

\hypertarget{explaining-the-results}{%
\subsubsection{Explaining the results}\label{explaining-the-results}}

FastQC shows a summary of modules that were run on our data. On the left
there is a \textbf{green tick} if the module seems entirely normal, an
\textbf{orange exclamation mark} if results are slightly abnormal and a
\textbf{red cross} if they are very unsusual.

It's important to analyse these results in detail, because these three
symbols might not show the whole context.

\begin{quote}
{[}!warning{]} mislm da mas pngs od grafov v neki mapci v root, dej jih
namesto teh screenshotov \#\#\#\# Basic Statistics
\end{quote}

{[}{[}Pasted image 20250421121853.png{]}{]}

Nothing that hasn't been mentioned before, except the \texttt{\%GC}
number, which tells us the overall \textbf{GC (guanosine + citosine)
content} across all sequences.

It is the simplest known HI (homology independent) metric used as a
genome signiature. It can be easily calculated from sequence data alone.
It displays a huge variation across genomes and is reasonably constant
within a given genome

Based on the table below, it seems GC content in our sequences is valid.

{[}{[}Pasted image 20250422113052.png\textbar400{]}{]} *source:
www.researchgate.net/figure/GC-content-of-Human-Mouse-Drosophila-melanogaster-Caenorhabditis-elegans\_tbl1\_5485258?\_\_cf\_chl\_tk=CQJZOaB7o7m7DdJQ09Z\_pGqtyr92uh63l1kqDyk4nvI-1745314185-1.0.1.1-Js8Lw9JrJD2ALRLlK.cLmPitbjmIqgkt5bpJ\_4\_iZuc*

\begin{quote}
It is also worth noting that the manuals says the Basic Statistics
module never raises a warning or an error. \#\#\#\# Per base sequence
quality
\end{quote}

{[}{[}Pasted image 20250421121906.png{]}{]}

It shows a range of quality values across all bases at each position.
\textbf{Green area} are good quality calls, \textbf{orange area} are
calls of reasonable quality and \textbf{red area} are calls of poor
quality. The quality of calls will degrade as the run progresses, so the
drop in quality at the end, as seen above, is not uncommon.

This plot can alert us to whether there were any problems occuring
during sequencing. Our mean quality (blue line) is consistently in the
green area, which means our per base sequence quality of good.

\hypertarget{per-sequence-quality-scores}{%
\paragraph{Per sequence quality
scores}\label{per-sequence-quality-scores}}

{[}{[}Pasted image 20250421121926.png{]}{]}

This plot shows us the average quality score per read on the x-axis and
the number of sequences with that average on the y-axis. Most of our
reads have the highest quality scores.

\hypertarget{per-base-sequence-content}{%
\paragraph{Per base sequence content}\label{per-base-sequence-content}}

{[}{[}Pasted image 20250421121941.png{]}{]}

This module presents \textbf{the proportion of each base position for
which each of the 4 DNA bases had been called}. For RNA-seq the
beginning is usually all over the place, because of adapters added
during library preparation, although in our case there isn't a lot of
noise in the beginning, but rather at the end, where the \texttt{\%A}
starts growing rapidly.

This module will fail when the percentage difference between AT and GC
is greater that 20\% in any position.

\hypertarget{per-sequence-gc-content}{%
\paragraph{Per sequence GC content}\label{per-sequence-gc-content}}

{[}{[}Pasted image 20250421121950.png{]}{]}

This plot shows the GC distribution across all sequences (red) compared
to a theoretical normal distribution (blue). As I mentioned, our
organisms GC content is around 40\%. A shifted theoretical distribution
indicates this bias, but this is not the reason this module fails.

The module will show a failure when our GC distribution doesn't follow
the theoretical one. This could indicate \textbf{contamination} with
another organism within the library (broad peaks) or presence of
\textbf{over-expressed sequences} (sharp peaks).

\hypertarget{per-base-n-content}{%
\paragraph{Per base N content}\label{per-base-n-content}}

{[}{[}Pasted image 20250421121959.png{]}{]}

If a sequencer is unable to make a base call with sufficient confidence
then it will substitute an N rather than a conventional base. This plot
show that no N's were substituted in our sequences.

\hypertarget{sequence-length-distribution}{%
\paragraph{Sequence Length
Distribution}\label{sequence-length-distribution}}

{[}{[}Pasted image 20250421122009.png{]}{]}

This module generates a graph showing the distribution of read sizes.
Generally all reads will be the same length (prior to trimming and
preprocessing).

\hypertarget{sequence-duplication-levels}{%
\paragraph{Sequence Duplication
Levels}\label{sequence-duplication-levels}}

{[}{[}Pasted image 20250421122022.png{]}{]}

In a diverse library most sequences will occur only once in the final
set. A high level of duplication is likely to indicate some kind of
enrichment bias (e.g.~PCR over amplification).

There are quite a few duplicate sequences in our dataset, but I am not
sure how to interpret this.

\hypertarget{overrepresented-sequences}{%
\paragraph{Overrepresented sequences}\label{overrepresented-sequences}}

{[}{[}Pasted image 20250421122035.png{]}{]}

\textbf{Overrepresented sequences} are sequences that are found more
frequently than statistical models predict it should occur by chance in
a random distribution of sequences of that length.

This model lists all of the sequences which make up more than 0.1 \% of
the total, but it tracks only the \textbf{first 200,000 sequences},
therefore some overrepresented sequences might be missed. Minimum read
length is \textbf{25bp} and reads longer than \textbf{75bp} are
\textbf{truncated to 50bp}.

The module looks for matches in a database of common contaminants and
reports hits if they're found (\emph{possible source} column). If a
single sequence is overrepresented, this could mean that it is higly
biologically significant or that the library is contaminated. A hit
\textbf{may} indicate some form of contamination (e.g.~adapters).

There are not hits detected, which could mean that they are less common
primers or that some other artifact is present.

\hypertarget{adapter-content}{%
\paragraph{Adapter content}\label{adapter-content}}

{[}{[}Pasted image 20250421122129.png{]}{]}

This module shows the mean percentage of adapter content or polyA or
polyG tail. There was a very small percentage of adapters detected in
our reads, which is almost neglegible but will be dealt with in the next
section.

A much higher percent of polyA tails was detected. This is not
untypical, since our reads came from 3' ends of mRNA molecules and the
polyadenilation is a known chemical modification of pre-mRNA molecules.

\hypertarget{b-exchange-fastqc-results-with-your-colleagues-and-run-multiqc-to-get-a-joined-report-for-all-datasets}{%
\subsection{b) Exchange FastQC results with your colleagues and run
MultiQC to get a joined report for all
datasets}\label{b-exchange-fastqc-results-with-your-colleagues-and-run-multiqc-to-get-a-joined-report-for-all-datasets}}

\hypertarget{checking-quality-of-sequences}{%
\section{Checking quality of
sequences}\label{checking-quality-of-sequences}}

\hypertarget{questions}{%
\subsection{Questions}\label{questions}}

\hypertarget{a-how-is-a-fastq-file-composed}{%
\subsubsection{a) How is a fastq file
composed?}\label{a-how-is-a-fastq-file-composed}}

FastQ files are text files that combine \textbf{FASTA formatted
sequences} with their \textbf{quality scores} and is the standard format
for storing the output of high-throughput sequencing instruments.

Since FastQ files bundle quality scores of sequences, we can take a
quick look at the first stored sequence with
\texttt{head\ -n\ 4\ SRR30833097.fastq}. This will output

\begin{verbatim}
@SRR30833097.1 NB500947:1144:HM5GMBGXH:1:11101:4455:1091 length=86
GGCGGTCGAGTGCCTCACAGTGTATCAAGGGTNGGCCACGNTCCTNACTAATNGNGGCTNNTTGCGCCATCGTCTCANGCAATGTT
+SRR30833097.1 NB500947:1144:HM5GMBGXH:1:11101:4455:1091 length=86
AAAAAEEEEEEEEEEEEEEEEEEEE<E//EEE#EEEEEE<#EEEE#E/AEEE#/#EEEE##EEE/EEAEEEEAE6EE#/EEEAAEA
\end{verbatim}

Each entry starts with \texttt{@} and is followed by a sequence
identifier and some other information about the sequence. The line
directly below it shows the raw sequence. The \texttt{+} line again can
contain information about the sequence and below are the quality values
for each nucleotide.

\hypertarget{b-how-can-i-count-the-number-of-reads-in-a-fastq-file-describe-different-ways-to-perform-that.}{%
\subsubsection{b) How can I count the number of reads in a fastq file?
Describe different ways to perform
that.}\label{b-how-can-i-count-the-number-of-reads-in-a-fastq-file-describe-different-ways-to-perform-that.}}

\hypertarget{bash}{%
\paragraph{BASH}\label{bash}}

Using \texttt{grep} and \texttt{wc}. Grep is used to search for patterns
using regular expression in a file. With
\texttt{\textquotesingle{}\^{}@\textquotesingle{}} we search for every
line that starts (\texttt{\^{}}) with \texttt{@}. It outputs every line
that matches our pattern. We pipe this in \texttt{wc\ -l}, as we used
before, to count the number of lines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep} \AttributeTok{{-}e} \StringTok{\textquotesingle{}\^{}@\textquotesingle{}}\NormalTok{ SRR30833097.fastq }\KeywordTok{|} \FunctionTok{wc} \AttributeTok{{-}l}
\end{Highlighting}
\end{Shaded}

\hypertarget{awk}{%
\paragraph{AWK}\label{awk}}

The AWK language is useful for pattern scanning and text proccessing. It
proccesses files line by line just like \texttt{grep}. In this case we
don't look for a certain identifier, but we use the fact that every
sequence occupies 4 lines to our advantage.

\texttt{NR} (number of records) is a builtin awk-variable. It records
lines processed so far. When reading line by line from file, this is
essentially the current line number.

With \texttt{NR\ \%\ 4\ ==\ 1} we calculate if the current line is the
first line of 4. This is true for lines 1, 5, 9\ldots{} which are lines
that contain \texttt{@} and represent one sequence each.

The filtered lines are returned, which we can again pipe into
\texttt{wc\ -l}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{awk} \StringTok{\textquotesingle{}NR \% 4 == 1\textquotesingle{}}\NormalTok{ SRR30833097.fastq }\KeywordTok{|} \FunctionTok{wc} \AttributeTok{{-}l}
\end{Highlighting}
\end{Shaded}

\hypertarget{python}{%
\paragraph{Python}\label{python}}

I made two scripts, one that uses pattern matching like the BASH command
and one that uses calculating the mod of line numbers. They are a little
more verbose than the other two options, but still pretty
straightforward.

I also measured time needed to process the file to determine which
approach is faster.

\hypertarget{pattern-matching}{%
\subparagraph{Pattern matching}\label{pattern-matching}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lines.py}

\ImportTok{import}\NormalTok{ sys}
\ImportTok{from}\NormalTok{ time }\ImportTok{import}\NormalTok{ time}

\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(sys.argv) }\OperatorTok{!=} \DecValTok{2}\NormalTok{:}
    \ControlFlowTok{raise} \PreprocessorTok{Exception}\NormalTok{(}\StringTok{"Usage: python3 lines.py \textless{}fastq{-}file\textgreater{}"}\NormalTok{)}

\NormalTok{fname }\OperatorTok{=}\NormalTok{ sys.argv[}\DecValTok{1}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Processing file..."}\NormalTok{)}
\NormalTok{start\_time }\OperatorTok{=}\NormalTok{ time()}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(fname, }\StringTok{"r"}\NormalTok{) }\ImportTok{as} \BuiltInTok{file}\NormalTok{:}
\NormalTok{    n }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in} \BuiltInTok{file}\NormalTok{:}
        \ControlFlowTok{if}\NormalTok{ line[}\DecValTok{0}\NormalTok{] }\OperatorTok{==} \StringTok{"@"}\NormalTok{:}
\NormalTok{            n }\OperatorTok{+=} \DecValTok{1}

\NormalTok{end\_time }\OperatorTok{=}\NormalTok{ time()}
\NormalTok{final\_time }\OperatorTok{=}\NormalTok{ end\_time }\OperatorTok{{-}}\NormalTok{ start\_time}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"number of sequences in file: }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"time: }\SpecialCharTok{\{}\NormalTok{final\_time}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The program outputs:

\begin{verbatim}
$ python3 lines.py SRR30833097.fastq
Processing file...
number of sequences in file: 4537365
time: 6.220864295959473 sec
\end{verbatim}

\hypertarget{line-count-calculation}{%
\subparagraph{Line count calculation}\label{line-count-calculation}}

These are the only altered lines.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nlines.py}
\CommentTok{\# snip}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(fname, }\StringTok{"r"}\NormalTok{) }\ImportTok{as} \BuiltInTok{file}\NormalTok{:}
\NormalTok{    data }\OperatorTok{=} \BuiltInTok{file}\NormalTok{.readlines()}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(data) }\OperatorTok{/} \DecValTok{4}

\CommentTok{\# snip}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$ python3 nlines.py SRR30833097.fastq
Processing file...
number of sequences in file: 4537365.0
time: 4.813834190368652 sec
\end{verbatim}

The second approach was faster by about 1.4 seconds, which is a
difference, but it's not huge, since we are already using a file with 18
million lines. The first one may be more robust since it doesn't make
assumptions about line content, while the second one \emph{assumes} we
have lines that can be evenly divided by 4.

\hypertarget{c-instead-of-python}{%
\paragraph{C instead of Python?}\label{c-instead-of-python}}

Since Python is slow by its nature, maybe a simple C program will do
this faster.

\hypertarget{pattern-matching-1}{%
\subparagraph{Pattern matching}\label{pattern-matching-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// clines.c}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdlib.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}sys/time.h\textgreater{}}

\PreprocessorTok{\#define MAX\_LINE\_LEN }\DecValTok{100}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ argc}\OperatorTok{,} \DataTypeTok{char} \OperatorTok{*}\NormalTok{argv}\OperatorTok{[])} \OperatorTok{\{}
  \ControlFlowTok{if} \OperatorTok{(}\NormalTok{argc }\OperatorTok{!=} \DecValTok{2}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Usage: ./clines.out \textless{}fastq{-}file\textgreater{}"}\OperatorTok{);}
\NormalTok{    exit}\OperatorTok{(}\DecValTok{1}\OperatorTok{);}
  \OperatorTok{\}}

  \DataTypeTok{FILE} \OperatorTok{*}\NormalTok{file }\OperatorTok{=}\NormalTok{ fopen}\OperatorTok{(}\NormalTok{argv}\OperatorTok{[}\DecValTok{1}\OperatorTok{],} \StringTok{"r"}\OperatorTok{);}

  \ControlFlowTok{if} \OperatorTok{(}\NormalTok{file }\OperatorTok{==}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    perror}\OperatorTok{(}\StringTok{"Error opening file."}\OperatorTok{);}
\NormalTok{    exit}\OperatorTok{(}\DecValTok{1}\OperatorTok{);}
  \OperatorTok{\}}

  \DataTypeTok{int}\NormalTok{ n }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}               \CommentTok{// line counter}
  \DataTypeTok{char}\NormalTok{ line}\OperatorTok{[}\NormalTok{MAX\_LINE\_LEN}\OperatorTok{];} \CommentTok{// line buffer}

  \CommentTok{// for measuring time more precisely than time\_t}
  \KeywordTok{struct}\NormalTok{ timeval start\_time}\OperatorTok{;}
  \KeywordTok{struct}\NormalTok{ timeval end\_time}\OperatorTok{;}
  \DataTypeTok{double}\NormalTok{ final\_time }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}

\NormalTok{  printf}\OperatorTok{(}\StringTok{"Processing file...}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\OperatorTok{);}
\NormalTok{  gettimeofday}\OperatorTok{(\&}\NormalTok{start\_time}\OperatorTok{,}\NormalTok{ NULL}\OperatorTok{);}

  \ControlFlowTok{while} \OperatorTok{(}\NormalTok{fgets}\OperatorTok{(}\NormalTok{line}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\NormalTok{line}\OperatorTok{),}\NormalTok{ file}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{line}\OperatorTok{[}\DecValTok{0}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}@\textquotesingle{}}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{      n}\OperatorTok{++;}
    \OperatorTok{\}}
  \OperatorTok{\}}

\NormalTok{  gettimeofday}\OperatorTok{(\&}\NormalTok{end\_time}\OperatorTok{,}\NormalTok{ NULL}\OperatorTok{);}
  \CommentTok{// first term is difference in seconds}
  \CommentTok{// second term is diference in microseconds converted to seconds}
\NormalTok{  final\_time }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{double}\OperatorTok{)(}\NormalTok{end\_time}\OperatorTok{.}\NormalTok{tv\_sec }\OperatorTok{{-}}\NormalTok{ start\_time}\OperatorTok{.}\NormalTok{tv\_sec}\OperatorTok{)} \OperatorTok{+}
               \OperatorTok{(}\DataTypeTok{double}\OperatorTok{)(}\NormalTok{end\_time}\OperatorTok{.}\NormalTok{tv\_usec }\OperatorTok{{-}}\NormalTok{ start\_time}\OperatorTok{.}\NormalTok{tv\_usec}\OperatorTok{)} \OperatorTok{/} \FloatTok{1000000.0}\OperatorTok{;}

\NormalTok{  printf}\OperatorTok{(}\StringTok{"sequences in file: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ n}\OperatorTok{);}
\NormalTok{  printf}\OperatorTok{(}\StringTok{"time: }\SpecialCharTok{\%f}\StringTok{ sec}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ final\_time}\OperatorTok{);}

  \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

When we compile and run this program, we get this:

\begin{verbatim}
$ gcc clines.c -o clines.out && ./clines SRR30833097.fastq
Processing file...
sequences in file: 4537365
time: 1.321521 sec
\end{verbatim}

Now this is a great improvement from the Python scripts. Will the second
approach with calculating the mod of lines be faster?

\hypertarget{line-count-calculation-1}{%
\subparagraph{Line count calculation}\label{line-count-calculation-1}}

These are the only altered lines.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{// nclines.c}
  \CommentTok{// snip}
  \ControlFlowTok{if} \OperatorTok{(}\NormalTok{argc }\OperatorTok{!=} \DecValTok{2}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Usage: ./nclines.out \textless{}fastq{-}file\textgreater{}"}\OperatorTok{);}
\NormalTok{    exit}\OperatorTok{(}\DecValTok{1}\OperatorTok{);}
  \OperatorTok{\}}
  \CommentTok{// snip}
  \ControlFlowTok{while} \OperatorTok{(}\NormalTok{fgets}\OperatorTok{(}\NormalTok{line}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\NormalTok{line}\OperatorTok{),}\NormalTok{ file}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    n}\OperatorTok{++;}
  \OperatorTok{\}}
\NormalTok{  n }\OperatorTok{=}\NormalTok{ n }\OperatorTok{/} \DecValTok{4}\OperatorTok{;}
  \CommentTok{// snip}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$ gcc nclines.c -o nclines.out && ./nclines SRR30833097.fastq
Processing file...
sequences in file: 4537365
time: 1.296595 sec
\end{verbatim}

It's faster by a fraction. Again, not a big difference, but faster
nevertheless, since we skip the comparisons for every line and do only
one operation more after we read the whole file.

\hypertarget{c-what-about-the-quality-of-your-reads}{%
\subsubsection{c) What about the quality of your
reads?}\label{c-what-about-the-quality-of-your-reads}}

Modules that failed:

\begin{itemize}
\tightlist
\item
  per base sequence content
\item
  per base GC content
\item
  adapter content
\end{itemize}

The overall quality can be better, which is what we adress in the next
section.

\hypertarget{d-describe-your-fastqc-andor-multiqc-and-interpret-the-results}{%
\subsubsection{\texorpdfstring{d)
\href{https://mugenomicscore.missouri.edu/PDF/FastQC_Manual.pdf}{Describe
your fastqc and/or multiqc and interpret the
results}}{d) Describe your fastqc and/or multiqc and interpret the results}}\label{d-describe-your-fastqc-andor-multiqc-and-interpret-the-results}}

My fastqc report was explained above in the
{[}{[}practical-notebook\#Checking quality of sequences\textbar Checking
quality of sequences{]}{]} section.

Multiqc report\ldots{}

\hypertarget{filtering-and-trimming-based-on-quality-parameters}{%
\section{Filtering and trimming based on quality
parameters}\label{filtering-and-trimming-based-on-quality-parameters}}

This is the \textbf{preprocessing} step, where we try and make the
quality of our reads better using
\href{https://github.com/OpenGene/fastp}{fastp}, an ultra-fast
all-in-one FASTQ preprocessor. based on the previously generated FastQC
report.

First install fastp with conda.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ install }\AttributeTok{{-}c}\NormalTok{ bioconda fastp}
\end{Highlighting}
\end{Shaded}

The program filters our reads and generates a \texttt{fastq} file and a
\texttt{html} report. We can try running the tool without any flags and
check the quality of processed reads.

\hypertarget{minimal-trimming}{%
\subsection{Minimal trimming}\label{minimal-trimming}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}minimal.fq}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}minimal.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication} \CommentTok{\# save time and memory}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-i} specifies the input fastq file
\item
  \texttt{-o} specifies name of the output fastq file
\item
  \texttt{-\/-html} specifies name for the output fastp report
\item
  \texttt{-\/-dont\_eval\_duplication} to save time and memory by not
  evaluating duplication rate of reads
\end{itemize}

The program outputs some text in the terminal. If we inspect it, we can
inspect quality of reads before and after filtering.

\begin{verbatim}
...
No adapter detected for read1

Read1 before filtering:
total reads: 4537365
total bases: 390213390
Q20 bases: 359416837(92.1078%)
Q30 bases: 349585493(89.5883%)

Read1 after filtering:
total reads: 4445266
total bases: 382292876
Q20 bases: 355260023(92.9288%)
Q30 bases: 345871060(90.4728%)

Filtering result:
reads passed filter: 4445266
reads failed due to low quality: 90121
reads failed due to too many N: 1978
reads failed due to too short: 0
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
...
\end{verbatim}

\begin{itemize}
\tightlist
\item
  no adapters were detected, which isn't very good, since FastQC
  detected some, but it will be dealt with in the next steps
\item
  after filtering quality went up for almost 1\%
\item
  around 90k reads had too low quality
\item
  around 2k reads were too long
\end{itemize}

So about 92k reads were removed and our quality increased only by almost
1\%. Let's run fastqc again on these filtered reads, to see if they
pass.

\begin{quote}
If you missed the program's output in the terminal, you can open the
generated HTML in the browser.
\end{quote}

Runing fastqc on newly generated fastq file as before:

{[}{[}Pasted image 20250426172831.png{]}{]}

We can see that the polyA tail situation hasn't improved. This is why we
need to add some flags to our \texttt{fastp} program, so it can do a
better job at filtering low quality reads.

\hypertarget{trim-polyx-tails}{%
\subsection{Trim polyX tails}\label{trim-polyx-tails}}

We can try with \texttt{-\/-trim\_poly\_x}, which should (according to
the documentation) trim the tails of our reads, where \texttt{x}
represents any base.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolyx.fq}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolyx.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4446923
total bases: 370914387
Q20 bases: 344936270(92.9962%)
Q30 bases: 335932934(90.5689%)

Filtering result:
reads passed filter: 4446923
reads failed due to low quality: 86186
reads failed due to too many N: 1944
reads failed due to too short: 2312
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 716650
bases trimmed in polyX tail: 12111332
...
\end{verbatim}

Quality improved for almost 0.1\%, which is not great progress. Many
reads were detected with polX tail at the 3' end.

{[}{[}Pasted image 20250426174128.png{]}{]}

A little better, but not good enough.

\hypertarget{trim-polyg-and-polyx-tails}{%
\subsection{Trim polyG and polyX
tails}\label{trim-polyg-and-polyx-tails}}

PolyX tail trimming is disabled by default, but is similar to polyG tail
trimming. PolyG tails can happen with Illumina NextSeq (which is the
machine that was used to sequence our reads), since \texttt{G} means no
signal in the Illumina two-color systems. If both trimming options are
enabled, fastp will first do polyG trimming and then polyX.

Since we don't have a great amount (or any, its hard to see) of reads
with a polyG tail, I don't think this will improve the quality greatly,
but it doesn't make it worse, so I think it's fine to add to our
command.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygx.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygx.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4443750
total bases: 370455963
Q20 bases: 344591801(93.0183%)
Q30 bases: 335614022(90.5948%)

Filtering result:
reads passed filter: 4443750
reads failed due to low quality: 85378
reads failed due to too many N: 1943
reads failed due to too short: 6294
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 669709
bases trimmed in polyX tail: 11402404
...
\end{verbatim}

There were less reads with polyX tails detected, probably because the
polyG trimming was done first.

{[}{[}Pasted image 20250426174210.png{]}{]}

\hypertarget{adjust-minimum-polyx-tail-length}{%
\subsection{Adjust minimum polyX tail
length}\label{adjust-minimum-polyx-tail-length}}

We still have to add some options to our command. We can try adjusting
minimum polyX tail length used for detecting. Default is 10. We can try
with 5.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygx5.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}poly\_x\_min\_len}\NormalTok{ 5}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygx5.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4445653
total bases: 370983902
Q20 bases: 345103461(93.0238%)
Q30 bases: 336143486(90.6086%)

Filtering result:
reads passed filter: 4445653
reads failed due to low quality: 84287
reads failed due to too many N: 1948
reads failed due to too short: 5477
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 897389
bases trimmed in polyX tail: 10973224
...
\end{verbatim}

The quality increased by about 0.1\% which is something, but not much.

\begin{itemize}
\tightlist
\item
  Less reads failed due to being too short.
\item
  A lot more reads were detected with polyX tail, which seems logical
  since we lowered the min tail size, but less were trimmed, which seems
  paradoxical.
\end{itemize}

The FastQC report shows that truly less reads were trimmed and
consequently the polyA tail percentage is higher. The following picture
shows two overlapping images.

{[}{[}Pasted image 20250426181750.png{]}{]}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We can also try to make the minimum length bigger, but I doubt it will
benefit the quality. Let's make it 15 and check the results.

\begin{verbatim}
...
Read1 after filtering:
total reads: 4442425
total bases: 373018357
Q20 bases: 346884901(92.9941%)
Q30 bases: 337799434(90.5584%)

Filtering result:
reads passed filter: 4442425
reads failed due to low quality: 86689
reads failed due to too many N: 1957
reads failed due to too short: 6294
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 401716
bases trimmed in polyX tail: 8684854
...
\end{verbatim}

As expected, even less reads were trimmed and the quality didn't
improve.

\hypertarget{use-a-sliding-window}{%
\subsection{Use a sliding window}\label{use-a-sliding-window}}

We have to look for alternative options for tail trimming. We can use
the \texttt{-\/-cut\_tail} flag. This will create a sliding window from
3' to 5' end and drop bases in the window, if their mean quality drops
below some threshold, otherwise it will stop trimming.

The window size is set using \texttt{-\/-cut\_window\_size} and is 4 by
default. The threshold is set using
\texttt{-\/-cut\_tail\_mean\_quality} and is 20 by default, must be
between 1 and 36.

Let's see what it does without changing these parameters.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygxwindow.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}cut\_tail}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygxwindow.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4458454
total bases: 359655463
Q20 bases: 338369588(94.0816%)
Q30 bases: 329735098(91.6808%)

Filtering result:
reads passed filter: 4458454
reads failed due to low quality: 62919
reads failed due to too many N: 1833
reads failed due to too short: 14159
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 969006
bases trimmed in polyX tail: 17650157
...
\end{verbatim}

The quality increased more than 1\%, which means we are on the right
track. A lot more reads were trimmed, but less failed due to low
quality. This could mean that it first cuts low quality bases in the
window.

\begin{quote}
The documentation has a ``features'' section where each feature is
numbered. This option is before polyG and X tail trimming, which could
mean that it does this first.
\end{quote}

What does FastQC show us?

{[}{[}Pasted image 20250426183302.png{]}{]}

The line dropped again, but it's still just a little bit over the
modules threshold for failure, which is 10\%. We can play around with
the parameters to get a better quality.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

First, let's try to make the window a bigger size. It says that it stops
if the mean quality is more than the threshold, which could in theory
mean that it stops immeadiately on the first 4 bases. Let's set the
window size to 10 with \texttt{-\/-cut\_window\_size\ 10}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygxwindow20.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}cut\_tail}
\ExtensionTok{{-}{-}cut\_window\_size}\NormalTok{ 10}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygxwindow20.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4463315
total bases: 358814939
Q20 bases: 338128014(94.2347%)
Q30 bases: 329597651(91.8573%)

Filtering result:
reads passed filter: 4463315
reads failed due to low quality: 52540
reads failed due to too many N: 1749
reads failed due to too short: 19761
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 981280
bases trimmed in polyX tail: 17829600
...
\end{verbatim}

The quality increased again, but not by much, even though a lot of reads
failed due to low quality. Our reads are not very long, so we shouldn't
increase this too much, but let's try with size 20.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygxwindow20.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}cut\_tail}
\ExtensionTok{{-}{-}cut\_window\_size}\NormalTok{ 20}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygxwindow20.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4469047
total bases: 362654545
Q20 bases: 340736791(93.9563%)
Q30 bases: 332102803(91.5755%)

Filtering result:
reads passed filter: 4469047
reads failed due to low quality: 41519
reads failed due to too many N: 1675
reads failed due to too short: 25124
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 877220
bases trimmed in polyX tail: 15206635
...
\end{verbatim}

The quality dropped again, since less tails were trimmed. Since we are
scanning a bigger window, good quality bases can be included in the mean
quality calculation and consequently low quality tails are saved from
trimming.

\hypertarget{increase-the-threshold-in-the-sliding-window}{%
\subsection{Increase the threshold in the sliding
window}\label{increase-the-threshold-in-the-sliding-window}}

Let's keep window size at 10 and increase the mean quality threshold
from default 20 to max 36. This could potentially eliminate too many
bases, but it would trim all reads with low quality tails.

\begin{quote}
{[}!warning{]} Although it says that max is 36 on the github page, the
command will fail with this value and print:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{ERROR:}\NormalTok{ the mean quality requirement for cutting by quality }\ErrorTok{(}\ExtensionTok{{-}{-}cut\_mean\_quality}\KeywordTok{)} \ExtensionTok{should}\NormalTok{ be 1 \textasciitilde{} 30, suggest 15 \textasciitilde{} 20.}
\end{Highlighting}
\end{Shaded}

So we will set it to 30.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out{-}trimpolygxwindow10meanquality30.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}cut\_tail}
\ExtensionTok{{-}{-}cut\_window\_size}\NormalTok{ 10}
\ExtensionTok{{-}{-}cut\_mean\_quality}\NormalTok{ 30}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report{-}trimpolygxwindow10meanquality30.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
...
Read1 after filtering:
total reads: 4473488
total bases: 335974371
Q20 bases: 322232306(95.9098%)
Q30 bases: 315263902(93.8357%)

Filtering result:
reads passed filter: 4473488
reads failed due to low quality: 13029
reads failed due to too many N: 1065
reads failed due to too short: 49783
reads with adapter trimmed: 0
bases trimmed due to adapters: 0
reads with polyX in 3' end: 1345574
bases trimmed in polyX tail: 19454923
...
\end{verbatim}

Now this is real progress. The quality increased by about 2\%. It
trimmed about 1.5M more reads. A lot more reads failed due to being too
short, which I don't know how to interpret.

The small adapter content present also went down, which is great.

{[}{[}Pasted image 20250426191615.png{]}{]}

\begin{quote}
{[}!Note{]} \emph{per base sequence content} and \emph{per sequence GC
content} These two modules still fail, but if we think logically, the
first one can be dependent on the other. Since our reads have skewed
distribution of bases, the second one fails and with it the first one
fails too, but this isn't critical in my opinion.
\end{quote}

\hypertarget{final-command}{%
\subsection{Final command}\label{final-command}}

Now we can remove all generated reports and filtered copies that we
won0t need anymore. You're free to keep them, but I think our working
directory will be more organized if we keep only one copy.

Remove all files whose names are matched with the pattern
\texttt{fastp*}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{ fastp}\PreprocessorTok{*}
\end{Highlighting}
\end{Shaded}

The final command used to filter our fastq data is

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp}
\ExtensionTok{{-}i}\NormalTok{ SRR30833097.fastq}
\ExtensionTok{{-}o}\NormalTok{ fastp{-}out.fq}
\ExtensionTok{{-}{-}trim\_poly\_g}
\ExtensionTok{{-}{-}trim\_poly\_x}
\ExtensionTok{{-}{-}cut\_tail}
\ExtensionTok{{-}{-}cut\_window\_size}\NormalTok{ 10}
\ExtensionTok{{-}{-}cut\_mean\_quality}\NormalTok{ 30}
\ExtensionTok{{-}{-}html}\NormalTok{ fastp{-}report.html}
\ExtensionTok{{-}{-}dont\_eval\_duplication}
\end{Highlighting}
\end{Shaded}

\hypertarget{downloading-the-reference-genome-and-an-annotation-file}{%
\section{Downloading the reference genome and an annotation
file}\label{downloading-the-reference-genome-and-an-annotation-file}}

The reference genome and its annotation can be downloaded using NCBI
command line tools. Install them with

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ install conda{-}forge::ncbi{-}datasets{-}cli}
\end{Highlighting}
\end{Shaded}

To find its accession number, got to
\href{https://www.ncbi.nlm.nih.gov/}{NCBI} and search \emph{drosophila
melanogaster}. Click on the genome section:

{[}{[}Pasted image 20250517181538.png\textbar400{]}{]}

Copy and echo the RefSeq/GenBank accession number into a file:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{echo} \StringTok{"GCF\_000001215.4"} \OperatorTok{\textgreater{}}\NormalTok{ RefGenAcc.txt}
\end{Highlighting}
\end{Shaded}

To download the reference genome execute the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{datasets}\NormalTok{ download genome }\DataTypeTok{\textbackslash{}}
\OperatorTok{\textgreater{}}\NormalTok{ accession }\VariableTok{$(}\FunctionTok{cat}\NormalTok{ RefGenAcc.txt}\VariableTok{)} \DataTypeTok{\textbackslash{}}
\OperatorTok{\textgreater{}}\NormalTok{ {-}{-}include genome,rna,protein,cds,gff3,gtf }\DataTypeTok{\textbackslash{}}
\OperatorTok{\textgreater{}}\NormalTok{ {-}{-}filename genome.zip}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{genome} : since we want to download a genome
\item
  \texttt{accession} : is clear
\item
  \texttt{-\/-include} : to specify what data files to include in the
  download
\item
  \texttt{-\/-filename} : desired name of the zip file
\end{itemize}

Unzip the downloaded file, rename it to something else and remove the
zip:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unzip}\NormalTok{ genome.zip }\CommentTok{\# unzips into "ncbi\_dataset/" directory }
\FunctionTok{mv}\NormalTok{ ncbi\_dataset ref\_genome}
\FunctionTok{rm}\NormalTok{ genome.zip}
\end{Highlighting}
\end{Shaded}

\hypertarget{questions-1}{%
\subsection{Questions}\label{questions-1}}

\hypertarget{a-describe-the-gtf}{%
\subsubsection{a) Describe the GTF}\label{a-describe-the-gtf}}

GTF is short for Gene Transfer Format. It's a tab-separated format. It's
based on the general feature format (GFF) but contains additional gene
information. It holds information about gene structure. Version 2 of GFF
is identical to GTF.

There are 9 attributes with each row being a feature. Empty attributes
should be donoted with a dot.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9423}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
column
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
explained
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
seqname & name of the chromosome/scaffold. Can be given with a name or
an ID (acc. number) and should match sequence names in the corresponding
sequence file \\
source & name of the program that generated this feature or the data
source (database or project name). \\
feature & name of the feature (e.g.~gene, transcript, exon\ldots). \\
start & start position of the feature (base pair). \\
end & end position of the feature (base pair). \\
score & floating point value (no scores in our gtf). \\
strand & \texttt{+} defines a forward and \texttt{-} a reverse
strand. \\
frame & one of 0, 1 or 2. Indicates the first base of the feature is the
first base of a codon, 1 that the second base is the first base of a
codon\ldots{} \\
attribute & a semicolon-separated list of tag-value pairs giving
additional information about a feature. \\
\end{longtable}

Example from our GTF:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0973}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0531}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0885}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0531}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0531}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0442}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0531}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0442}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.5133}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
seqname
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
start
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
end
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
score
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
strand
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
frame
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
attribute
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NC\_004354.4 & RefSeq & gene & 122493 & 122706 & . & + & . & gene\_id
``Dmel\_CR40469''; transcript\_id ````; cyt\_map''1A1-\ldots{} \\
NC\_004354.4 & RefSeq & transcript & 122493 & 122706 & . & + & . &
gene\_id ``Dmel\_CR40469''; transcript\_id ``NR\_003723.2'';
db\ldots{} \\
NC\_004354.4 & RefSeq & exon & 122493 & 122706 & . & + & . & gene\_id
``Dmel\_CR40469''; transcript\_id ``NR\_003723.2''; db\ldots{} \\
\end{longtable}

\hypertarget{b-examine-gtf-files.-which-information-can-be-found-in-these-files}{%
\subsubsection{b) Examine GTF files. Which information can be found in
these
files?}\label{b-examine-gtf-files.-which-information-can-be-found-in-these-files}}

In our GTF file everything except the score and frame could be found.

\hypertarget{c-how-many-genes-are-present}{%
\subsubsection{c) How many genes are
present?}\label{c-how-many-genes-are-present}}

We can use grep to skip lines with \texttt{\#} (comments) and then use
AWK to check if third column matches ``gene'':

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ genomic.gtf }\KeywordTok{|} \FunctionTok{grep} \AttributeTok{{-}v} \StringTok{"\#"} \KeywordTok{|} \FunctionTok{awk} \StringTok{\textquotesingle{}$3=="gene"\textquotesingle{}} \KeywordTok{|} \FunctionTok{wc} \AttributeTok{{-}l}
\end{Highlighting}
\end{Shaded}

\texttt{-v} of grep selects non-matching lines and \texttt{\$3} extracts
the third column.

\hypertarget{d-provide-me-the-commands-and-results-for-counting-the-number-of-sequences-in-the-various-fasta-files-dna-rna-protein.}{%
\subsubsection{d) Provide me the commands and results for counting the
number of sequences in the various fasta files (DNA, RNA,
protein).}\label{d-provide-me-the-commands-and-results-for-counting-the-number-of-sequences-in-the-various-fasta-files-dna-rna-protein.}}

Commands and programs are found in the section
{[}{[}practical-notebook\#b) How can I count the number of reads in a
fastq file? Describe different ways to perform that.{]}{]}.

\hypertarget{e-describe-differences-between-different-genomic-fasta-files.}{%
\subsubsection{e) Describe differences between different genomic fasta
files.}\label{e-describe-differences-between-different-genomic-fasta-files.}}

Protein fasta cointains protein sequences (amino acid residues).

\begin{verbatim}
$ head -n 2 protein.faa
>NP_001007096.1 uncharacterized protein Dmel_CG42637, isoform C [Drosophila melanogaster]
MTRWPFNLLLLLSVAVRDCSNHRTVLTVGYLTALTGDLKTRQGLAISGALTMALDEVNKDPNLLPNVYLDLRWNDTKGDT
\end{verbatim}

RNA fasta contains transcripts (mRNA).

\begin{verbatim}
$ head -n 2 rna.fna
>NM_001007095.3 Drosophila melanogaster uncharacterized protein, transcript variant C (CG42637), mRNA
TCACATATTCAAAATCGGGTAGGTAGTCGCGACGGAAAACGGGAAACGCGGACGAATCGCGGAGCCAGAGAAGCGGTAAA
\end{verbatim}

CDS contains nucleotide coding sequences (exons).

\begin{verbatim}
$ head -n 4 cds_from_genomic.fna
>lcl|NC_004354.4_cds_NP_001096854.1_1 [gene=CG17636] [locus_tag=Dmel_CG17636] [db_xref=FLYBASE:FBpp0111834,GeneID:5740847] [protein=uncharacterized protein, isoform A] [protein_id=NP_001096854.1] [location=complement(join(124464..125409,125495..126259,126626..126630))] [gbkey=CDS]
ATGTCGTGCTGCAAGAAGTACGCCGTCTGCTGGATTATCCTGGTGGTGACCGCATTGGGTGTGACCTTGGGTCTGGTTTT
\end{verbatim}

\hypertarget{alignment-using-hisat2}{%
\section{Alignment using HISAT2}\label{alignment-using-hisat2}}

HISAT2 is an alignment program for mapping NGS reads against a single
reference genome. It outputs alignments in the SAM format. It will build
an index over our genome.

Install it with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ install }\AttributeTok{{-}c}\NormalTok{ bioconda hisat2}
\end{Highlighting}
\end{Shaded}

The command we'll use is \texttt{hisat2-build}, which will index our
reference genome. It takes 2 parameters:

\begin{itemize}
\tightlist
\item
  \texttt{reference\_in} : comma-separated list of files with ref
  sequences.
\item
  \texttt{hisat2\_index\_base} : writes output files to this
  \emph{base}name.
\end{itemize}

In the reference genome directory run:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{hisat2{-}build} \DataTypeTok{\textbackslash{}}
\NormalTok{GCF\_000001251.4\_Release\_6\_plus\_ISO1\_MT\_genomic.fna }\DataTypeTok{\textbackslash{}}
\NormalTok{genome}
\end{Highlighting}
\end{Shaded}

The indexes are created in \texttt{genome.X.ht2} files. After creating
an index, we can use the \texttt{hisat2} command to create pairwise
alignments:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{hisat2} \AttributeTok{{-}x}\NormalTok{ genome }\AttributeTok{{-}U}\NormalTok{ ../../../fastp{-}out.fq }\AttributeTok{{-}S}\NormalTok{ HisatAlignment.sam}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-x\ \textless{}ht2-idx\textgreater{}} : prefix of files with
  indexes.
\item
  \texttt{-U\ \textless{}r\textgreater{}} : files with unpaired reads.
\item
  \texttt{-S\ \textless{}sam\textgreater{}} : file for SAM output.
\end{itemize}

\hypertarget{quantifying-the-expression-of-transcripts-using-rna--seq-data-with-salmon}{%
\section{Quantifying the expression of transcripts using RNA- seq data
with
Salmon}\label{quantifying-the-expression-of-transcripts-using-rna--seq-data-with-salmon}}

{[}{[}quantifying the expression of transcripts using RNA-seq data with
salmon{]}{]}

\end{document}
